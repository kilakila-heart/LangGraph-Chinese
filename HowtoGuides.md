# How-to guides(æ“ä½œæŒ‡å—)
[å®˜ç½‘](https://langchain-ai.github.io/langgraph/how-tos/#how-to-guides)

Welcome to the LangGraph how-to guides! These guides provide practical, step-by-step instructions for accomplishing key tasks in LangGraph.
æ¬¢è¿æ¥åˆ°LangGraphçš„æ“ä½œæŒ‡å—ï¼Œæœ¬æŒ‡å—æä¾›å®ç”¨çš„ï¼Œä¸€æ­¥ä¸€æ­¥çš„ä»‹ç»æ¥å®ŒæˆLangGraphä¸­çš„å…³é”®ä»»åŠ¡ã€‚

## å¯æ§æ€§
LangGraph is known for being a highly controllable agent framework. These how-to guides show how to achieve that controllability.

- [How to create branches for parallel execution](https://langchain-ai.github.io/langgraph/how-tos/branching/)
- [How to create map-reduce branches for parallel execution](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/)
- [How to control graph recursion limit](https://langchain-ai.github.io/langgraph/how-tos/recursion-limit/)



## Persistence[Â¶](https://langchain-ai.github.io/langgraph/how-tos/#persistence)

LangGraph makes it easy to persist state across graph runs (thread-level persistence) and across threads (cross-thread persistence). These how-to guides show how to add persistence to your graph.

- [How to add thread-level persistence to your graph](https://langchain-ai.github.io/langgraph/how-tos/persistence/)
- [How to add cross-thread persistence to your graph](https://langchain-ai.github.io/langgraph/how-tos/cross-thread-persistence/)
- [How to use Postgres checkpointer for persistence](https://langchain-ai.github.io/langgraph/how-tos/persistence_postgres/)
- [How to create a custom checkpointer using MongoDB](https://langchain-ai.github.io/langgraph/how-tos/persistence_mongodb/)
- [How to create a custom checkpointer using Redis](https://langchain-ai.github.io/langgraph/how-tos/persistence_redis/)



## Memory[Â¶](https://langchain-ai.github.io/langgraph/how-tos/#memory)

LangGraph makes it easy to manage conversation [memory](https://langchain-ai.github.io/langgraph/concepts/memory/) in your graph. These how-to guides show how to implement different strategies for that.

- [How to manage conversation history](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/)
- [How to delete messages](https://langchain-ai.github.io/langgraph/how-tos/memory/delete-messages/)
- [How to add summary conversation memory](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/)



## Human in the Loop[Â¶](https://langchain-ai.github.io/langgraph/how-tos/#human-in-the-loop)

One of LangGraph's main benefits is that it makes human-in-the-loop workflows easy. These guides cover common examples of that.

- [How to add breakpoints](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/breakpoints/)
- [How to add dynamic breakpoints](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/dynamic_breakpoints/)
- [How to edit graph state](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/edit-graph-state/)
- [How to wait for user input](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/wait-user-input/)
- [How to view and update past graph state](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/time-travel/)
- [Review tool calls](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/review-tool-calls/)



## Streamingæµå¼ä¼ è¾“

 [å‚è€ƒæ–‡æ¡£](https://langchain-ai.github.io/langgraph/how-tos/#streaming)

LangGraphé¦–å…ˆè¢«æ„å»ºä¸ºæµå¼äº¤äº’ã€‚æœ¬æŒ‡å—å±•ç¤ºæ€æ ·ä½¿ç”¨ä¸åŒçš„æµå¼streamingæ¨¡å¼ã€‚

- [æ€æ ·æµå¼å¤„ç†å®Œæ•´çš„graphçŠ¶æ€state](#How-to-stream-full-state-of-your-graph)
- [æ€æ ·ä½¿ç”¨updatesæ¨¡å¼æµå¼å¤„ç†graphçŠ¶æ€](#How-to-stream-state-updates-of-your-graph)
- [æ€æ ·äºå¤§æ¨¡å‹è¿›è¡Œsteamå¤„ç†](#How-to-stream-LLM-tokens-from-your-graph)
- [æ€æ ·åœ¨æ²¡æœ‰Langchainæ¨¡å‹ä¸‹äºå¤§æ¨¡å‹è¿›è¡Œsteamå¤„ç†](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens-without-langchain/)
- [æ€æ ·steamä¼ è¾“è‡ªå®šä¹‰æ•°æ®](https://langchain-ai.github.io/langgraph/how-tos/streaming-content/)
- [æ€æ ·åŒæ—¶é…ç½®å¤šsteamå¼ä¼ è¾“](https://langchain-ai.github.io/langgraph/how-tos/stream-multiple/)
- [How to stream events from within a tool](https://langchain-ai.github.io/langgraph/how-tos/streaming-events-from-within-tools/)
- [How to stream events from within a tool without LangChain models](https://langchain-ai.github.io/langgraph/how-tos/streaming-events-from-within-tools-without-langchain/)
- [How to stream events from the final node æ€æ ·ä»æœ€ç»ˆèŠ‚ç‚¹ä¼ è¾“(stream)äº‹ä»¶](https://langchain-ai.github.io/langgraph/how-tos/streaming-from-final-node/)
- [How to stream from subgraphs](https://langchain-ai.github.io/langgraph/how-tos/streaming-subgraphs/)
- [How to disable streaming for models that don't support it](https://langchain-ai.github.io/langgraph/how-tos/disable-streaming/)



###  æ€æ ·æµå¼å¤„ç†å®Œæ•´çš„graphçŠ¶æ€ï¼ˆvaluesï¼‰ {#How-to-stream-full-state-of-your-graph}

[æºæ–‡æ¡£ï¼šHow to stream full state of your graph](https://langchain-ai.github.io/langgraph/how-tos/stream-values/#how-to-stream-full-state-of-your-graph)

LangGraphæ”¯æŒå¤šç§æµæ¨¡å¼ã€‚ä¸»è¦çš„æœ‰:

values:  è¿™ç§æµå¼æ¨¡å¼è¿”å›graphçš„å€¼ã€‚è¿™æ˜¯æ¯ä¸€ä¸ªèŠ‚ç‚¹è¢«è°ƒç”¨å**å®Œæ•´çš„graphçŠ¶æ€ï¼ˆstateï¼‰**ã€‚
updatesï¼šè¿™ç§æµå¼æ¨¡å¼è¿”å›graphæ›´æ–°åçš„å†…å®¹ã€‚è¿™æ˜¯æ¯ä¸€ä¸ªèŠ‚ç‚¹è¢«è°ƒç”¨å**æ›´æ–°åçš„graphçŠ¶æ€ï¼ˆstateï¼‰**

æœ¬æŒ‡å—åŒ…å«äº†æµå¼æ¨¡å¼`stream_mode="values"`

#### å‡†å¤‡

é¦–å…ˆï¼ŒæŒ‰ç…§ç›¸å…³ä¾èµ–åŒ…å’Œè®¾ç½®ä½ è‡ªå·±çš„API keyã€‚

```python
%%capture --no-stderr
%pip install -U langgraph langchain-openai langchain-community
import getpass
import os


def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")


_set_env("OPENAI_API_KEY")
```

>Set up [LangSmith](https://smith.langchain.com/) for LangGraph development

> Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph â€” read more about how to get started [here](https://docs.smith.langchain.com/).



####  å®šä¹‰graph
[æºæ–‡æ¡£é“¾æ¥](https://langchain-ai.github.io/langgraph/how-tos/stream-values/#define-the-graph)

æœ¬ç¤ºä¾‹æˆ‘ä»¬å°†ä½¿ç”¨ç®€å•çš„ReActæ¨¡å¼çš„agentã€‚
```python
from typing import Literal
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.runnables import ConfigurableField
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent


@tool
def get_weather(city: Literal["nyc", "sf"]):
    """Use this to get weather information."""
    if city == "nyc":
        return "It might be cloudy in nyc"
    elif city == "sf":
        return "It's always sunny in sf"
    else:
        raise AssertionError("Unknown city")


tools = [get_weather]

model = ChatOpenAI(model_name="gpt-4o", temperature=0)
graph = create_react_agent(model, tools)
```
å‚è€ƒAPI: [TavilySearchResults](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html) | [ConfigurableField](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.utils.ConfigurableField.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [create_react_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

####  Stream valuesæ¨¡å¼

[æºæ–‡æ¡£é“¾æ¥](https://langchain-ai.github.io/langgraph/how-tos/stream-values/#stream-values)



```python
inputs = {"messages": [("human", "what's the weather in sf")]}
async for chunk in graph.astream(inputs, stream_mode="values"):
    chunk["messages"][-1].pretty_print()
```
```text
================================[1m Human Message [0m=================================

what's the weather in sf
==================================[1m Ai Message [0m==================================
Tool Calls:
  get_weather (call_61VvIzqVGtyxcXi0z6knZkjZ)
 Call ID: call_61VvIzqVGtyxcXi0z6knZkjZ
  Args:
    city: sf
=================================[1m Tool Message [0m=================================
Name: get_weather

It's always sunny in sf
==================================[1m Ai Message [0m==================================

The weather in San Francisco is currently sunny.
```

å¦‚æœæˆ‘ä»¬åªæƒ³è·å–æœ€ç»ˆç»“æœï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é€šç”¨çš„æ–¹æ³•å¹¶åªéœ€è¦è¿½è¸ªæˆ‘ä»¬æ¥æ”¶åˆ°çš„æœ€åä¸€ä¸ªå€¼
```python
inputs = {"messages": [("human", "what's the weather in sf")]}
async for chunk in graph.astream(inputs, stream_mode="values"):
    final_result = chunk
```
```python
final_result
```
```text
{'messages': [HumanMessage(content="what's the weather in sf", id='54b39b6f-054b-4306-980b-86905e48a6bc'),
  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_avoKnK8reERzTUSxrN9cgFxY', 'function': {'arguments': '{"city":"sf"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5e6c71d4a8', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f2f43c89-2c96-45f4-975c-2d0f22d0d2d1-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_avoKnK8reERzTUSxrN9cgFxY'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71}),
  ToolMessage(content="It's always sunny in sf", name='get_weather', id='fc18a798-c7b2-4f73-84fa-8ffdffb6ddcb', tool_call_id='call_avoKnK8reERzTUSxrN9cgFxY'),
  AIMessage(content='The weather in San Francisco is currently sunny. Enjoy the sunshine!', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 84, 'total_tokens': 98}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_5e6c71d4a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-21418147-da8e-4738-a076-239377397c40-0', usage_metadata={'input_tokens': 84, 'output_tokens': 14, 'total_tokens': 98})]}
```

```python
final_result["messages"][-1].pretty_print()
```

```tex
==================================[1m Ai Message [0m==================================

The weather in San Francisco is currently sunny. Enjoy the sunshine!
```



###  æ€æ ·æµå¼å¤„ç†graphçš„çŠ¶æ€æ›´æ–°(updates) {#How-to-stream-state-updates-of-your-graph}

 [æºæ–‡æ¡£How to stream state updates of your graph](https://langchain-ai.github.io/langgraph/how-tos/stream-updates/#how-to-stream-state-updates-of-your-graph)

LangGraphæ”¯æŒå¤šç§æµæ¨¡å¼ã€‚ä¸»è¦çš„æœ‰:

values:  è¿™ç§æµå¼æ¨¡å¼è¿”å›graphçš„å€¼ã€‚è¿™æ˜¯æ¯ä¸€ä¸ªèŠ‚ç‚¹è¢«è°ƒç”¨å**å®Œæ•´çš„graphçŠ¶æ€ï¼ˆstateï¼‰**ã€‚
updatesï¼šè¿™ç§æµå¼æ¨¡å¼è¿”å›graphæ›´æ–°åçš„å†…å®¹ã€‚è¿™æ˜¯æ¯ä¸€ä¸ªèŠ‚ç‚¹è¢«è°ƒç”¨å**æ›´æ–°åçš„graphçŠ¶æ€ï¼ˆstateï¼‰**

æœ¬æŒ‡å—åŒ…å«äº†æµå¼æ¨¡å¼`stream_mode="updates"`ã€‚
#### å‡†å¤‡

é¦–å…ˆï¼ŒæŒ‰ç…§ç›¸å…³ä¾èµ–åŒ…å’Œè®¾ç½®ä½ è‡ªå·±çš„API keyã€‚

```python
%%capture --no-stderr
%pip install -U langgraph langchain-openai langchain-community
import getpass
import os


def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")


_set_env("OPENAI_API_KEY")
```

>Set up [LangSmith](https://smith.langchain.com/) for LangGraph development

> Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph â€” read more about how to get started [here](https://docs.smith.langchain.com/).

####  å®šä¹‰graph
[æºæ–‡æ¡£é“¾æ¥](https://langchain-ai.github.io/langgraph/how-tos/stream-values/#define-the-graph)

æœ¬ç¤ºä¾‹æˆ‘ä»¬å°†ä½¿ç”¨ç®€å•çš„ReActæ¨¡å¼çš„agentã€‚
```python
from typing import Literal
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.runnables import ConfigurableField
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import create_react_agent


@tool
def get_weather(city: Literal["nyc", "sf"]):
    """Use this to get weather information."""
    if city == "nyc":
        return "It might be cloudy in nyc"
    elif city == "sf":
        return "It's always sunny in sf"
    else:
        raise AssertionError("Unknown city")


tools = [get_weather]

model = ChatOpenAI(model_name="gpt-4o", temperature=0)
graph = create_react_agent(model, tools)
```
å‚è€ƒAPI: [TavilySearchResults](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html) | [ConfigurableField](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.utils.ConfigurableField.html) | [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) | [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html) | [create_react_agent](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)

####  Stream updatesæ¨¡å¼
[æºæ–‡æ¡£](https://langchain-ai.github.io/langgraph/how-tos/stream-updates/#stream-updates)

```python
inputs = {"messages": [("human", "what's the weather in sf")]}
async for chunk in graph.astream(inputs, stream_mode="updates"):
    for node, values in chunk.items():
        print(f"Receiving update from node: '{node}'")
        print(values)
        print("\n\n")
```
```text
Receiving update from node: 'agent'
{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_kc6cvcEkTAUGRlSHrP4PK9fn', 'function': {'arguments': '{"city":"sf"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 57, 'total_tokens': 71}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3e7d703517', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-cd68b3a0-86c3-4afa-9649-1b962a0dd062-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'call_kc6cvcEkTAUGRlSHrP4PK9fn'}], usage_metadata={'input_tokens': 57, 'output_tokens': 14, 'total_tokens': 71})]}



Receiving update from node: 'tools'
{'messages': [ToolMessage(content="It's always sunny in sf", name='get_weather', tool_call_id='call_kc6cvcEkTAUGRlSHrP4PK9fn')]}



Receiving update from node: 'agent'
{'messages': [AIMessage(content='The weather in San Francisco is currently sunny.', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 84, 'total_tokens': 94}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3e7d703517', 'finish_reason': 'stop', 'logprobs': None}, id='run-009d83c4-b874-4acc-9494-20aba43132b9-0', usage_metadata={'input_tokens': 84, 'output_tokens': 10, 'total_tokens': 94})]}
```



###  æ€æ ·åœ¨ä½ çš„graphä¸­ä½¿ç”¨æµå¼è¾“å‡ºLLMå¤§æ¨¡å‹çš„tokens {#How-to-stream-LLM-tokens-from-your-graph}
[å‚è€ƒæºæ–‡æ¡£ï¼š How to stream LLM tokens from your graph](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/#how-to-stream-llm-tokens-from-your-graph)
In this example we will stream tokens from the language model powering an agent. We will use a ReAct agent as an example.
åœ¨æœ¬ä¾‹å­ä¸­æˆ‘ä»¬å°†æµå¼è¾“å‡ºå¤§è¯­è¨€æ¨¡å‹çš„tokensæ¥å¢å¼ºagentã€‚æˆ‘ä»¬å°†ä½¿ç”¨ReActæ¨¡å¼çš„agentä½œä¸ºä¾‹å­ã€‚

æœ¬æŒ‡å—ç´§è·Ÿç€æœ¬ç›®å½•ä¸­çš„å…¶ä»–æŒ‡å—ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†ç”¨ä¸‹é¢çš„STREAMINGæ ‡ç­¾æŒ‡å‡ºä¸åŒä¹‹å¤„ï¼ˆå¦‚æœä½ åªæ˜¯æƒ³æœç´¢ä»–ä»¬çš„å·®å¼‚ï¼‰

æ³¨æ„

>In this how-to, we will create our agent from scratch to be transparent (but verbose). You can accomplish similar functionality using the create_react_agent(model, tools=tool) (API doc) constructor. This may be more appropriate if you are used to LangChainâ€™s AgentExecutor class.



å…³äº Python < 3.11çš„è¯´æ˜

>When using python 3.8, 3.9, or 3.10, please ensure you manually pass the RunnableConfig through to the llm when invoking it like so: llm.ainvoke(..., config). The stream method collects all events from your nested code using a streaming tracer passed as a callback. In 3.11 and above, this is automatically handled via contextvar's; prior to 3.11, asyncio's tasks lacked proper contextvar support, meaning that the callbacks will only propagate if you manually pass the config through. We do this in the call_model method below.

#### å‡†å¤‡
æˆ‘ä»¬é¦–å…ˆå®‰è£…ä¾èµ–åŒ…

```bash
%%capture --no-stderr
%pip install --quiet -U langgraph langchain_openai langsmith
```
Next, we need to set API keys for OpenAI (the LLM we will use).

```python
import getpass
import os


def _set_env(var: str):
    if not os.environ.get(var):
        os.environ[var] = getpass.getpass(f"{var}: ")


_set_env("OPENAI_API_KEY")
```
ä¸ºLangGraph å¼€å‘æ¨¡å¼è®¾ç½®LangSmith 

> Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph â€” read more about how to get started here.

#### è®¾ç½®çŠ¶æ€state
[å‚è€ƒæºæ–‡æ¡£ï¼šSet up the state](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/#set-up-the-state)


åœ¨ `langgraph` çš„ä¸»è¦ç±»æ˜¯ [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.StateGraph). è¿™ä¸ªgraphç”±`State`å¯¹è±¡å‚æ•°åŒ–ï¼Œå®ƒå°†ä¼ è¾“åˆ°æ¯ä¸€ä¸ªèŠ‚ç‚¹ã€‚ç„¶åæ¯ä¸ªèŠ‚ç‚¹è¿”å›æ“ä½œï¼Œè¿™æ ·graphå°±ç”¨æ¥`update`è¯¥çŠ¶æ€stateã€‚è¿™äº›æ“ä½œèƒ½å¤Ÿæ—¢èƒ½è®¾ç½®æŒ‡å®šçŠ¶æ€stateå±æ€§ï¼ˆä¾‹å¦‚è¦†ç›–å·²å­˜åœ¨çš„å€¼ï¼‰ï¼Œä¹Ÿèƒ½è¿½åŠ åˆ°å·²å­˜åœ¨çš„å±æ€§ä¸­ã€‚æ— è®ºæ˜¯é‡ç½®è¿˜æ˜¯è¿½åŠ éƒ½æ˜¯é€šè¿‡ä½ æ„é€ graphæ—¶å€™çš„`State`å¯¹è±¡æ¥æ ‡æ³¨çš„ã€‚

å¯¹è¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬ä»…ä»…å°†è®°å½•è¿™ä¸ªçŠ¶æ€æ¥ä½œä¸ºmessagesåˆ—è¡¨çš„ä¸€éƒ¨åˆ†ã€‚æˆ‘ä»¬å¸Œæœ›æ¯ä¸€ä¸ªèŠ‚ç‚¹ä»…ä»…æ·»åŠ åˆ°messagesçš„åˆ—è¡¨ä¸­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†ç”¨ä¸€ä¸ª`TypedDict`ä½œä¸ºä¸€ä¸ªkeyï¼ˆ`messages`ï¼‰æ¥æ ‡æ³¨å®ƒï¼Œæ‰€ä»¥`messages`å±æ€§æ˜¯â€ä»…è¿½åŠ â€œã€‚

```python
from typing import Annotated

from typing_extensions import TypedDict

from langgraph.graph.message import add_messages

# Add messages essentially does this with more
# robust handling
# def add_messages(left: list, right: list):
#     return left + right


class State(TypedDict):
    messages: Annotated[list, add_messages]
```

**API å‚è€ƒ:** [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages)

####  è®¾ç½®å·¥å…·tools
[å‚è€ƒæ–‡æ¡£ï¼šSet up the tools](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/#set-up-the-tools)


æˆ‘ä»¬é¦–å…ˆå®šä¹‰æˆ‘ä»¬æƒ³ä½¿ç”¨çš„å·¥å…·ã€‚å¯¹è¿™ä¸ªç®€å•ä¾‹å­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„æœç´¢å¼•æ“ã€‚è¿™èƒ½ç®€å•çš„åˆ›å»ºä½ è‡ªå·±çš„å·¥å…·ã€‚â€”â€”çœ‹[è¿™é‡Œ](https://python.langchain.com/docs/how_to/custom_tools)çš„æ–‡æ¡£æ˜¯æ€ä¹ˆåšçš„ã€‚

```python
from langchain_core.tools import tool


@tool
def search(query: str):
    """Call to surf the web."""
    # This is a placeholder, but don't tell the LLM that...
    return ["Cloudy with a chance of hail."]


tools = [search]
```

**API å‚è€ƒ:** [tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html)

æˆ‘ä»¬ç°åœ¨èƒ½ç”¨ç®€å•çš„ [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode)åŒ…è£…è¿™äº›å·¥å…·äº†ã€‚è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ç±»ï¼Œåœ¨messagesåˆ—è¡¨ä¸­åŒ…å«äº†å¸¦æœ‰[tool_callsçš„AIMessages]((https://api.python.langchain.com/en/latest/messages/langchain_core.messages.ai.AIMessage.html#langchain_core.messages.ai.AIMessage.tool_calls))ï¼Œè¿è¡Œè¿™ä¸ªå·¥å…·ï¼Œå¹¶è¿”å›è¾“å‡ºä½œä¸º[ToolMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.tool.ToolMessage.html#langchain_core.messages.tool.ToolMessage)

```python
from langgraph.prebuilt import ToolNode

tool_node = ToolNode(tools)
```

**API å‚è€ƒ:** [ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)



#### è®¾ç½®å¤§æ¨¡å‹model
[å‚è€ƒæ–‡æ¡£ï¼šSet up the model](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/#set-up-the-model)

Now we need to load the chat model we want to use. This should satisfy two criteria:

ç°åœ¨æˆ‘ä»¬éœ€è¦åŠ è½½æˆ‘ä»¬æƒ³ä½¿ç”¨çš„å¤§æ¨¡å‹ã€‚è¿™åº”è¯¥æ»¡è¶³ä¸¤ä¸ªæ ‡æ³¨ï¼š

 1.å®ƒåº”è¯¥èƒ½å¤„ç†æ¶ˆæ¯messagesï¼Œå› ä¸ºæˆ‘ä»¬çš„çŠ¶æ€stateä¸»è¦ç”±æ¶ˆæ¯åˆ—è¡¨messagesæ„æˆï¼ˆèŠå¤©å†å²ï¼‰
 2.å®ƒåº”èƒ½å¤„ç†å·¥å…·è°ƒç”¨ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨äº†é¢„æ„å»ºçš„[ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode)

**æ³¨æ„:** æ¨¡å‹ä¾èµ–ä¸æ˜¯ä½¿ç”¨LangGraphå¿…é¡»çš„ã€‚ â€”â€” å®ƒä»¬åªæ˜¯è¿™ä¸ªç‰¹æ®Šä¾‹å­çš„è¦æ±‚ã€‚

```python
from langchain_openai import ChatOpenAI

model = ChatOpenAI(model="gpt-3.5-turbo")
```

**API å‚è€ƒ:** [ChatOpenAI](https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)

After we've done this, we should make sure the model knows that it has these tools available to call. We can do this by converting the LangChain tools into the format for function calling, and then bind them to the model class.

æˆ‘ä»¬åšå®Œè¿™æ­¥åï¼Œæˆ‘ä»¬åº”è¯¥ç¡®ä¿å¤§æ¨¡å‹çŸ¥é“å®ƒç”±è¿™äº›å·¥å…·å¯ä»¥è°ƒç”¨ã€‚æˆ‘ä»¬å¯ä»¥ç”¨è¦†ç›–langchianå·¥å…·è½¬æ¢æˆå‡½æ•°è°ƒç”¨çš„æ ¼å¼å®ç°è¿™ä¸€ç‚¹ï¼Œç„¶åå°†ä»–ä»¬ç»‘å®šåˆ°modeçš„ç±»ä¸Šï¼Œå¦‚ä¸‹ä»£ç ï¼š

```python
model = model.bind_tools(tools)
```

#### å®šä¹‰èŠ‚ç‚¹nodes
[å‚è€ƒï¼šDefine the nodes](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/#define-the-nodes)

We now need to define a few different nodes in our graph. In `langgraph`, a node can be either a function or a [runnable](https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel). There are two main nodes we need for this:

1. The agent: responsible for deciding what (if any) actions to take.
2. A function to invoke tools: if the agent decides to take an action, this node will then execute that action.

We will also need to define some edges. Some of these edges may be conditional. The reason they are conditional is that based on the output of a node, one of several paths may be taken. The path that is taken is not known until that node is run (the LLM decides).

1. Conditional Edge: after the agent is called, we should either: a. If the agent said to take an action, then the function to invoke tools should be called b. If the agent said that it was finished, then it should finish
2. Normal Edge: after the tools are invoked, it should always go back to the agent to decide what to do next

Let's define the nodes, as well as a function to decide how what conditional edge to take.

ç°åœ¨æˆ‘ä»¬éœ€è¦åœ¨æˆ‘ä»¬çš„graphä¸­å®šä¹‰å‡ ä¸ªä¸åŒçš„èŠ‚ç‚¹ï¼Œåœ¨`langgraph`ä¸­ï¼Œä¸€ä¸ªèŠ‚ç‚¹å¯ä»¥æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ª[runnable](https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel). è¿™é‡Œæˆ‘ä»¬éœ€è¦ä¸¤ä¸ªä¸»è¦çš„èŠ‚ç‚¹ï¼š

1. agent èŠ‚ç‚¹ï¼šè¿”å›å†³å®šéœ€è¦é‡‡å–çš„è¡ŒåŠ¨ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚
2. ä¸€ä¸ªè°ƒç”¨å·¥å…·çš„å‡½æ•°ï¼šå¦‚æœagentéœ€è¦é‡‡å–è¡ŒåŠ¨ï¼Œè¿™ä¸ªèŠ‚ç‚¹å°†æ‰§è¡Œè¿™ä¸ªè¡ŒåŠ¨ã€‚

æˆ‘ä»¬ä¹Ÿéœ€è¦å®šä¹‰ä¸€äº›è¾¹ï¼ˆedgesï¼‰ã€‚æœ‰äº›è¾¹å¯èƒ½æ˜¯æ¡ä»¶è¾¹ã€‚ä»–ä»¬ä½œä¸ºæ¡ä»¶çš„ç†ç”±æ˜¯ï¼šä»¥èŠ‚ç‚¹è¾“å‡ºä¸ºåŸºç¡€ï¼Œå¯ä»¥é‡‡å–å‡ ç§è·¯å¾„çš„ä¸€ç§ã€‚ç›´åˆ°èŠ‚ç‚¹è¿è¡Œå‰è·¯å¾„è¢«é‡‡ç”¨æ˜¯æœªçŸ¥çš„ï¼ˆç”±å¤§æ¨¡å‹å†³å®šï¼‰ã€‚

1. æ¡ä»¶è¾¹:åœ¨agentè¢«è°ƒç”¨ä¹‹åï¼Œæˆ‘ä»¬åº”è¯¥å¦‚ä¸‹ä¸¤ç§åœºæ™¯ä¹‹ä¸€ï¼ša.å¦‚æœagentè¯´é‡‡å–è¡ŒåŠ¨ï¼Œé‚£ä¹ˆå‡½æ•°åº”è¯¥è¢«å·¥å…·è°ƒç”¨ã€‚b.å¦‚æœagentè¯´ç»“æŸäº†ï¼Œé‚£ä¹ˆæµç¨‹åº”è¯¥ç»“æŸã€‚ 
2. æ™®é€šè¾¹: åœ¨å·¥å…·è¢«è°ƒç”¨ä¹‹åï¼Œå®ƒåªä¼šè¿”å›ç»“æœç»™agentï¼Œç”±agentå†³å®šä¸‹ä¸€æ­¥éœ€è¦é‡‡å–ä»€ä¹ˆè¡ŒåŠ¨ã€‚

è®©æˆ‘ä»¬å®šä¹‰è¿™äº›èŠ‚ç‚¹ï¼Œä¹Ÿå®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥å†³å®šå¦‚ä½•é€‰æ‹©æ¡ä»¶è¾¹ã€‚

**STREAMING**

æˆ‘ä»¬å®šä¹‰æ¯ä¸€ä¸ªèŠ‚ç‚¹ä½œä¸ºå¼‚æ­¥(async)å‡½æ•°



å°æ³¨é‡Šï¼šæ‰‹å·¥å›è°ƒä¼ è¾“ Manual Callback Propagation

> Note that in `call_model(state: State, config: RunnableConfig):` below, we a) accept the [RunnableConfig](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.config.RunnableConfig.html#langchain_core.runnables.config.RunnableConfig) in the node and b) pass this in as the second arg for `llm.ainvoke(..., config)`. This is optional for python 3.11 and later.

```python
from typing import Literal

from langchain_core.runnables import RunnableConfig

from langgraph.graph import END, START, StateGraph


# Define the function that determines whether to continue or not
def should_continue(state: State):
    messages = state["messages"]
    last_message = messages[-1]
    # If there is no function call, then we finish
    if not last_message.tool_calls:
        return END
    # Otherwise if there is, we continue
    else:
        return "tools"


# Define the function that calls the model
async def call_model(state: State, config: RunnableConfig):
    messages = state["messages"]
    # Note: Passing the config through explicitly is required for python < 3.11
    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks
    response = await model.ainvoke(messages, config)
    # We return a list, because this will get added to the existing list
    return {"messages": response}
```

**API å‚è€ƒ:** [RunnableConfig](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html) | [END](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.END) | [START](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)



#### å®šä¹‰graph 
[å‚è€ƒï¼šDefine the graph](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/#define-the-graph)

æˆ‘ä»¬ç°åœ¨å°†ä»–ä»¬ç»„åˆèµ·æ¥å¹¶ä¸”å®šä¹‰graphï¼

```python
# Define a new graph
workflow = StateGraph(State)

# Define the two nodes we will cycle between
workflow.add_node("agent", call_model)
workflow.add_node("tools", tool_node)

# Set the entrypoint as `agent`
# This means that this node is the first one called
workflow.add_edge(START, "agent")

# We now add a conditional edge
workflow.add_conditional_edges(
    # First, we define the start node. We use `agent`.
    # This means these are the edges taken after the `agent` node is called.
    "agent",
    # Next, we pass in the function that will determine which node is called next.
    should_continue,
    # Next we pass in the path map - all the nodes this edge could go to
    ["tools", END],
)

workflow.add_edge("tools", "agent")

# Finally, we compile it!
# This compiles it into a LangChain Runnable,
# meaning you can use it as you would any other runnable
app = workflow.compile()
```
æŸ¥çœ‹å›¾çš„æµç¨‹
```python
from IPython.display import Image, display

display(Image(app.get_graph().draw_mermaid_png()))
```
<img src=".\images\steam_llm_graph.png" alt="å¯è§†åŒ–graphæµç¨‹" style="zoom:78%;" />

#### å¤§æ¨¡å‹æµå¼è¾“å‡ºTokens
[å‚è€ƒæ–‡æ¡£ï¼šStreaming LLM Tokens](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/#streaming-llm-tokens)



ä½ å¯ä»¥è®¿é—®å¤§æ¨¡å‹çš„è¾“å‡ºtokensï¼Œå› ä¸ºä»–ä»¬æ˜¯ç”±æ¯ä¸€ä¸ªèŠ‚ç‚¹è¾“å‡ºçš„ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ä»…ä»…åªæœ‰"agent"èŠ‚ç‚¹ç”Ÿäº§å‡ºå¤§æ¨¡å‹çš„tokensã€‚ä¸ºäº†èƒ½æ­£å¸¸å·¥ä½œï¼Œä½ å¿…é¡»ä½¿ç”¨æ”¯æŒæµå¼è¾“å‡ºçš„å¤§æ¨¡å‹ï¼Œå¹¶ä¸”åœ¨æ„é€ LLMçš„æ—¶å€™å°±è®¾ç½®è¿™ç§æ¨¡å¼ï¼ˆä¾‹å¦‚ï¼šChatOpenAI(model="gpt-3.5-turbo-1106", streaming=True)ï¼‰ã€‚

```python
from langchain_core.messages import AIMessageChunk, HumanMessage

inputs = [HumanMessage(content="what is the weather in sf")]
first = True
async for msg, metadata in app.astream({"messages": inputs}, stream_mode="messages"):
    if msg.content and not isinstance(msg, HumanMessage):
        print(msg.content, end="|", flush=True)

    if isinstance(msg, AIMessageChunk):
        if first:
            gathered = msg
            first = False
        else:
            gathered = gathered + msg

        if msg.tool_call_chunks:
            print(gathered.tool_calls)
```
```text
[{'name': 'search', 'args': {}, 'id': 'call_lfwgOci165GXplBjSDBeD4sE', 'type': 'tool_call'}]
[{'name': 'search', 'args': {}, 'id': 'call_lfwgOci165GXplBjSDBeD4sE', 'type': 'tool_call'}]
[{'name': 'search', 'args': {}, 'id': 'call_lfwgOci165GXplBjSDBeD4sE', 'type': 'tool_call'}]
[{'name': 'search', 'args': {'query': ''}, 'id': 'call_lfwgOci165GXplBjSDBeD4sE', 'type': 'tool_call'}]
[{'name': 'search', 'args': {'query': 'weather'}, 'id': 'call_lfwgOci165GXplBjSDBeD4sE', 'type': 'tool_call'}]
[{'name': 'search', 'args': {'query': 'weather in'}, 'id': 'call_lfwgOci165GXplBjSDBeD4sE', 'type': 'tool_call'}]
[{'name': 'search', 'args': {'query': 'weather in San'}, 'id': 'call_lfwgOci165GXplBjSDBeD4sE', 'type': 'tool_call'}]
[{'name': 'search', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_lfwgOci165GXplBjSDBeD4sE', 'type': 'tool_call'}]
[{'name': 'search', 'args': {'query': 'weather in San Francisco'}, 'id': 'call_lfwgOci165GXplBjSDBeD4sE', 'type': 'tool_call'}]
["Cloudy with a chance of hail."]|The| weather| in| San| Francisco| is| currently| cloudy| with| a| chance| of| hail|.|
```

**API å‚è€ƒ:** [AIMessageChunk](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessageChunk.html) | [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)